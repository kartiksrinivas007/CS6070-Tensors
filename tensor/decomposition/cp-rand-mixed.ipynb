{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensor.operation.kruskal import kruskal\n",
    "from tensor.operation.khatri_rao import khatri_rao\n",
    "from tensor.operation.matricize import matricize\n",
    "from tensor.operation.sampled_khatri_rao_prod import SKR\n",
    "from tensor.operation.sampled_matricize import Sampled_Matricize\n",
    "from tensor.operation.right_pseudo_inverse import right_pseudo_inverse\n",
    "import tensorly as tly\n",
    "from tensorly.tenalg import mode_dot\n",
    "from scipy.linalg import hadamard\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm: CP RAND\n",
    "\n",
    "$\\text{Input:}$ $\\mathcal{X} \\in \\mathbb{R}^{I_1 \\times \\cdots \\times I_N}$, $\\mathbf{R}$, $\\mathbf{S}$\n",
    "\n",
    "$\\text{Output:}$ $\\boldsymbol{\\lambda}$, $\\{\\mathbf{A}^{(n)}\\}$\n",
    "\n",
    "$\n",
    "\\textbf{CPRAND} (\\mathcal{X}, \\mathbf{R}, \\mathbf{S}):\\\\\n",
    "\\quad \\text{Initialize factor matrices } \\mathbf{A}^{(2)}, \\dots, \\mathbf{A}^{(N)}\\\\\n",
    "\\quad \\textbf{repeat}:\\\\\n",
    "\\quad \\quad \\text{for } n = 1, \\dots, N \\text{ do}:\\\\\n",
    "\\quad \\quad \\quad \\text{Define sampling operator} \\mathbf{S} \\in \\mathbb{R}^{S \\times \\prod_{m \\neq n} I_m}\\\\\n",
    "\\quad \\quad \\quad \\mathbf{Z}_S \\gets \\text{SKR}(\\mathbf{S}, \\mathbf{A}^{(1)}, \\dots, \\mathbf{A}^{(n-1)}, \\mathbf{A}^{(n+1)}, \\dots, \\mathbf{A}^{(N)})\\\\\n",
    "\\quad \\quad \\quad \\mathbf{X}^T_S \\gets \\mathbf{S} \\mathbf{X}^T_{(n)}\\\\\n",
    "\\quad \\quad \\quad \\mathbf{A}^{(n)} \\gets \\underset{\\mathbf{A}}{\\arg \\min} \\lVert \\mathbf{Z}_S \\mathbf{A}^T - \\mathbf{X}^T_S \\rVert_F\\\\\n",
    "\\quad \\quad \\quad \\text{Normalize columns of } \\mathbf{A}^{(n)} \\text{ and update } \\boldsymbol{\\lambda}\\\\\n",
    "\\quad \\quad \\textbf{end for}\\\\\n",
    "\\quad \\textbf{until convergence}\\\\\n",
    "\\quad \\text{Return } \\boldsymbol{\\lambda}, \\{\\mathbf{A}^{(n)}\\}\\\\\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CP_rand_mix:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tensor: np.ndarray,\n",
    "        rank: int,\n",
    "        max_iter=10000,\n",
    "        eps=0.01,\n",
    "        initalisation: str = \"random\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        PARAFAC decompositon, it initializes the maximum amount of iterations that\n",
    "        are done along with the tolerance value for the convergence\n",
    "        it also takes the size of the core tensor rank\n",
    "        \"\"\"\n",
    "        self.tensor = tensor\n",
    "        self.mixed_tensor = None\n",
    "\n",
    "        self.rank = rank\n",
    "        self.max_iter = max_iter\n",
    "        self.eps = eps\n",
    "        self.init_type = initalisation\n",
    "        self.errors = []\n",
    "\n",
    "        self.factors = []\n",
    "        self.mixed_factors = [None for _ in range(self.tensor.ndim)]\n",
    "\n",
    "        self.lamda = np.ones(self.rank)\n",
    "\n",
    "        # Diagonal matrices for sign-flip operation, diagnol entries = 1 or -1 randomly, size: (tensor.shape[i], tensor.shape[i])\n",
    "        self.D = []\n",
    "        for i in range(self.tensor.ndim):\n",
    "            diag_signs = np.random.choice([-1, 1], size=self.tensor.shape[i])\n",
    "            self.D.append(np.diag(diag_signs))\n",
    "            assert self.D[-1].shape == (self.tensor.shape[i], self.tensor.shape[i])\n",
    "\n",
    "        self.F = []\n",
    "        for i in range(self.tensor.ndim):\n",
    "            self.F.append(np.random.rand(self.tensor.shape[i], self.tensor.shape[i]))\n",
    "            self.F.append(hadamard(self.tensor.shape[i]))\n",
    "            # assert self.F[-1].shape == (self.tensor.shape[i], self.tensor.shape[i])\n",
    "\n",
    "    def fit(self, check_convergence=True):\n",
    "        self._init_factors()\n",
    "        for i in range(self.max_iter):\n",
    "            for mode in range(self.tensor.ndim):\n",
    "                self._update_factors(mode)\n",
    "            if check_convergence and self._is_converged():\n",
    "                break\n",
    "        print(f\"Converged in {i+1} iterations\")\n",
    "\n",
    "    def _init_factors(self):\n",
    "        \"\"\"\n",
    "        initialize the factors using the `rank` many left singular\n",
    "        vectors of the corresponding mode-n matricization of the input tensor\n",
    "        \"\"\"\n",
    "        if self.init_type == \"random\":\n",
    "            self.factors = [\n",
    "                np.random.rand(self.tensor.shape[i], self.rank)\n",
    "                for i in range(self.tensor.ndim)\n",
    "            ]\n",
    "        # elif self.init_type == 'hosvd':\n",
    "        #     self.factors = []\n",
    "        #     for i in range(self.tensor.ndim):\n",
    "        #         M = np.linalg.svd(matricize(self.tensor, i))[0]\n",
    "        #         if M.shape[1] < self.rank:\n",
    "        #             M_ = np.zeros((M.shape[0], self.rank - M.shape[1]))\n",
    "        #             M = np.concatenate((M, M_), axis=1)\n",
    "        #         else:\n",
    "        #             M = M[:, :self.rank]\n",
    "        #         self.factors.append(M)\n",
    "        else:\n",
    "            raise Exception(\"Invalid initialisation method\")\n",
    "\n",
    "        # mix the factor matrices\n",
    "        for i in range(self.tensor.ndim):\n",
    "            self.mixed_factors[i] = self.F[i] @ self.D[i] @ self.factors[i]\n",
    "            assert self.mixed_factors[i].shape == (self.tensor.shape[i], self.rank)\n",
    "\n",
    "        # mix the tensor\n",
    "        self.mixed_tensor = self.tensor\n",
    "        for i in range(self.tensor.ndim):\n",
    "            self.mixed_tensor = mode_dot(self.mixed_tensor, self.F[i] @ self.D[i], i)\n",
    "        assert self.mixed_tensor.shape == self.tensor.shape\n",
    "\n",
    "    def _update_factors(self, mode):\n",
    "        \"\"\"\n",
    "        Update the factors per iteration for the (mode+1)th Factor\n",
    "        \"\"\"\n",
    "        tot_row = 1  # total number of rows in the mode-{mode} matricization\n",
    "        for x in range(self.tensor.ndim):\n",
    "            if x != mode:\n",
    "                tot_row *= self.tensor.shape[x]\n",
    "        S = 10 * int(math.log(self.rank)) * self.rank\n",
    "\n",
    "        sel_rows = np.random.choice(tot_row, S, replace=True)\n",
    "\n",
    "        Z_s_hat = SKR(sel_rows, self.mixed_factors, mode)\n",
    "        assert Z_s_hat.shape == (S, self.rank)\n",
    "\n",
    "        # complex conjugate of F[mode]\n",
    "        F_conj = np.conj(self.F[mode])\n",
    "        assert F_conj.shape == (self.tensor.shape[mode], self.tensor.shape[mode])\n",
    "\n",
    "        X_s_hat = self.D[mode] @ F_conj @ Sampled_Matricize(sel_rows, self.mixed_tensor, mode)\n",
    "        assert X_s_hat.shape == (self.tensor.shape[mode], S)\n",
    "\n",
    "        rpi = right_pseudo_inverse(Z_s_hat.T)\n",
    "        assert rpi.shape == (S, self.rank)\n",
    "        self.factors[mode] = X_s_hat @ rpi\n",
    "        assert self.factors[mode].shape == (self.tensor.shape[mode], self.rank)\n",
    "\n",
    "        col_norms = np.linalg.norm(self.factors[mode], axis=0)\n",
    "\n",
    "        self.factors[mode] = self.factors[mode] / col_norms # normalize the self.factors[mode]\n",
    "        self.mixed_factors[mode] = self.F[mode] @ self.D[mode] @ self.factors[mode]\n",
    "\n",
    "        self.lamda = col_norms\n",
    "\n",
    "    def _is_converged(self):\n",
    "        \"\"\"\n",
    "        check if the algorithm has converged\n",
    "        by computing the Frobenius norm of the difference between the current tensor\n",
    "        and the tensor obtained by multiplying the factors\n",
    "        \"\"\"\n",
    "        tmp = self.factors[0]\n",
    "\n",
    "        self.factors[0] = self.factors[0] * self.lamda\n",
    "        estim = kruskal(*self.factors)\n",
    "        error = np.linalg.norm(self.tensor - estim)\n",
    "        print(\"Error = \", error)\n",
    "        self.errors.append(error)\n",
    "        self.factors[0] = tmp\n",
    "        return error < self.eps\n",
    "\n",
    "    def plot_errors(self):\n",
    "        plt.plot(self.errors)\n",
    "        plt.legend([\"Errors\"])\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Frobenius norm\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error is: 1.0005808618753766e-14\n"
     ]
    },
    {
     "ename": "UFuncTypeError",
     "evalue": "Cannot cast ufunc 'multiply' output from dtype('complex128') to dtype('float64') with casting rule 'same_kind'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/aayush/Desktop/AAYUSH/COLLEGE/SEMESTERS/SEMESTER 6/Tensors/CS6070-Tensors/cp-rand-mixed.ipynb Cell 4\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aayush/Desktop/AAYUSH/COLLEGE/SEMESTERS/SEMESTER%206/Tensors/CS6070-Tensors/cp-rand-mixed.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError is: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(X \u001b[39m-\u001b[39m X_tly)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aayush/Desktop/AAYUSH/COLLEGE/SEMESTERS/SEMESTER%206/Tensors/CS6070-Tensors/cp-rand-mixed.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m solver \u001b[39m=\u001b[39m CP_rand_mix(X, R, \u001b[39mint\u001b[39m(\u001b[39m1e4\u001b[39m), \u001b[39m0.01\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrandom\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aayush/Desktop/AAYUSH/COLLEGE/SEMESTERS/SEMESTER%206/Tensors/CS6070-Tensors/cp-rand-mixed.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m solver\u001b[39m.\u001b[39;49mfit()\n",
      "\u001b[1;32m/Users/aayush/Desktop/AAYUSH/COLLEGE/SEMESTERS/SEMESTER 6/Tensors/CS6070-Tensors/cp-rand-mixed.ipynb Cell 4\u001b[0m in \u001b[0;36mCP_rand_mix.fit\u001b[0;34m(self, check_convergence)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aayush/Desktop/AAYUSH/COLLEGE/SEMESTERS/SEMESTER%206/Tensors/CS6070-Tensors/cp-rand-mixed.ipynb#W3sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aayush/Desktop/AAYUSH/COLLEGE/SEMESTERS/SEMESTER%206/Tensors/CS6070-Tensors/cp-rand-mixed.ipynb#W3sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39mfor\u001b[39;00m mode \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensor\u001b[39m.\u001b[39mndim):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aayush/Desktop/AAYUSH/COLLEGE/SEMESTERS/SEMESTER%206/Tensors/CS6070-Tensors/cp-rand-mixed.ipynb#W3sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_factors(mode)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aayush/Desktop/AAYUSH/COLLEGE/SEMESTERS/SEMESTER%206/Tensors/CS6070-Tensors/cp-rand-mixed.ipynb#W3sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39mif\u001b[39;00m check_convergence \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_converged():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aayush/Desktop/AAYUSH/COLLEGE/SEMESTERS/SEMESTER%206/Tensors/CS6070-Tensors/cp-rand-mixed.ipynb#W3sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;32m/Users/aayush/Desktop/AAYUSH/COLLEGE/SEMESTERS/SEMESTER 6/Tensors/CS6070-Tensors/cp-rand-mixed.ipynb Cell 4\u001b[0m in \u001b[0;36mCP_rand_mix._update_factors\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aayush/Desktop/AAYUSH/COLLEGE/SEMESTERS/SEMESTER%206/Tensors/CS6070-Tensors/cp-rand-mixed.ipynb#W3sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m S \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m \u001b[39m*\u001b[39m \u001b[39mint\u001b[39m(math\u001b[39m.\u001b[39mlog(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrank)) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrank\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aayush/Desktop/AAYUSH/COLLEGE/SEMESTERS/SEMESTER%206/Tensors/CS6070-Tensors/cp-rand-mixed.ipynb#W3sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m sel_rows \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(tot_row, S, replace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aayush/Desktop/AAYUSH/COLLEGE/SEMESTERS/SEMESTER%206/Tensors/CS6070-Tensors/cp-rand-mixed.ipynb#W3sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m Z_s_hat \u001b[39m=\u001b[39m SKR(sel_rows, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmixed_factors, mode)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aayush/Desktop/AAYUSH/COLLEGE/SEMESTERS/SEMESTER%206/Tensors/CS6070-Tensors/cp-rand-mixed.ipynb#W3sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m \u001b[39massert\u001b[39;00m Z_s_hat\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (S, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrank)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aayush/Desktop/AAYUSH/COLLEGE/SEMESTERS/SEMESTER%206/Tensors/CS6070-Tensors/cp-rand-mixed.ipynb#W3sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m \u001b[39m# complex conjugate of F[mode]\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/AAYUSH/COLLEGE/SEMESTERS/SEMESTER 6/Tensors/CS6070-Tensors/tensor/operation/sampled_khatri_rao_prod.py:36\u001b[0m, in \u001b[0;36mSKR\u001b[0;34m(S, factors, mode)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[39m# for tmp in range(S_sz):\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[39m#     ret[tmp,:]=np.multiply(ret[tmp,:],factors[n][indices[tmp,n],:])\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     ret \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m factors[n][indices[:, n], :]\n\u001b[1;32m     37\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: Cannot cast ufunc 'multiply' output from dtype('complex128') to dtype('float64') with casting rule 'same_kind'"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(2, 2, 2)\n",
    "R = 4\n",
    "tly_ans = tly.decomposition.parafac(X, rank=R)\n",
    "X_tly = tly.cp_to_tensor(tly_ans)\n",
    "print(f\"Error is: {np.linalg.norm(X - X_tly)}\")\n",
    "solver = CP_rand_mix(X, R, int(1e4), 0.01, \"random\")\n",
    "solver.fit()\n",
    "# print(factors[0].shape, factors[1].shape, factors[2].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EE5606_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
