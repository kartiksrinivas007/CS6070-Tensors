{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensor.operation.kruskal import kruskal\n",
    "from tensor.operation.khatri_rao import khatri_rao\n",
    "from tensor.operation.matricize import matricize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CP decomposition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm: CP decomposition ALS\n",
    "\n",
    "$\\text{Input:} \\quad \\text{Tensor } X \\in \\mathbb{R}^{J_1 \\times J_2 \\times \\cdots \\times J_N}, \\text{ rank } R$\n",
    "\n",
    "$\\text{Output:} \\quad \\text{Factor matrices } U_1, U_2, \\cdots, U_N \\\\\n",
    "$\n",
    "\n",
    "\\begin{align*}\n",
    "&\\text{Initialize } U_1, U_2, \\cdots, U_N \\text{ with random matrices} \\\\\n",
    "&\\text{while } \\text{not converged } \\text{do} \\\\\n",
    "& \\quad \\quad \\text{for } n = 1, 2, \\cdots, N \\text{ do} \\\\\n",
    "& \\quad \\quad \\quad \\text{Compute } \\mathbf{A}^{(n)^T} = \\left( \\mathbf{A}^{(N)} \\odot \\mathbf{A}^{(N-1)} \\odot \\cdots \\odot \\mathbf{A}^{(n+1)} \\odot \\mathbf{A}^{(n-1)} \\cdot \\mathbf{A}^{(1)} \\right )^{\\dag} X^T_{(n)} \\\\\n",
    "& \\quad \\quad \\text{end for} \\\\\n",
    "&\\text{end while} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpDecomposition(X: np.ndarray, rank: int, maxIter: int = 5, tol: float = 1e-4, initialisation: str = 'random'):\n",
    "    \"\"\"cpDecomposition performs CP decomposition of a tensor X using alternating least squares.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Tensor to be decomposed.\n",
    "        rank (int): Rank of the decomposition.\n",
    "        maxIter (int, optional): Maximum number of iterations. Defaults to 1000.\n",
    "        tol (float, optional): Tolerance for the stopping criterion. Defaults to 1e-6.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Factor matrices of the decomposition.\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialize factor matrices\n",
    "    if initialisation == 'random':\n",
    "        U = [np.random.rand(X.shape[i], rank) for i in range(X.ndim)]\n",
    "    elif initialisation == 'hosvd':\n",
    "        U = []\n",
    "        for i in range(X.ndim):\n",
    "            M = np.linalg.svd(matricize(X, i))[0]\n",
    "            if M.shape[1] < rank:\n",
    "                M_ = np.zeros((M.shape[0], rank - M.shape[1]))\n",
    "                M = np.concatenate((M, M_), axis=1)\n",
    "            else:\n",
    "                M = M[:, :rank]\n",
    "            print(\"M\", M.shape)\n",
    "            U.append(M)\n",
    "    else:\n",
    "        print(\"Invalid initialisation method\")\n",
    "        return\n",
    "\n",
    "    for i in range(X.ndim):\n",
    "        print(i, U[i].shape , matricize(X, i).shape)\n",
    "\n",
    "    # Iterate until convergence\n",
    "    for itr in range(maxIter):\n",
    "\n",
    "        for i in range(X.ndim):\n",
    "\n",
    "            khatriRaoProd = np.ones((1, rank))\n",
    "            for j in range(X.ndim, 0, -1):\n",
    "                if j != (i + 1):\n",
    "                    khatriRaoProd = khatri_rao(khatriRaoProd, U[j - 1])\n",
    "\n",
    "            U[i] = matricize(X, i) @ np.linalg.pinv(khatriRaoProd).T\n",
    "        print(\"Iteration \", itr+1, \" completed. loss =\", np.linalg.norm(X - kruskal(*U)))\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(X - kruskal(*U)) < tol:\n",
    "            break\n",
    "\n",
    "    return np.array(U)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (3, 2) (3, 9)\n",
      "1 (3, 2) (3, 9)\n",
      "2 (3, 2) (3, 9)\n",
      "Iteration  1  completed. loss = 3.121962667675004\n",
      "Iteration  2  completed. loss = 3.042200831869479\n",
      "Iteration  3  completed. loss = 3.022234971950612\n",
      "Iteration  4  completed. loss = 3.0111868639556714\n",
      "Iteration  5  completed. loss = 3.0039591341283014\n",
      "Iteration  6  completed. loss = 2.9992241502438355\n",
      "Iteration  7  completed. loss = 2.9962067916565633\n",
      "Iteration  8  completed. loss = 2.99434329508674\n",
      "Iteration  9  completed. loss = 2.9932205026531005\n",
      "Iteration  10  completed. loss = 2.9925525870755396\n",
      "Iteration  11  completed. loss = 2.9921538706293638\n",
      "Iteration  12  completed. loss = 2.9919099209457793\n",
      "Iteration  13  completed. loss = 2.991753003431649\n",
      "Iteration  14  completed. loss = 2.9916441883991554\n",
      "Iteration  15  completed. loss = 2.991561550330812\n",
      "Iteration  16  completed. loss = 2.991492888488279\n",
      "Iteration  17  completed. loss = 2.991431436127014\n",
      "Iteration  18  completed. loss = 2.991373408537754\n",
      "Iteration  19  completed. loss = 2.991316630533991\n",
      "Iteration  20  completed. loss = 2.991259777651536\n",
      "Iteration  21  completed. loss = 2.99120195840381\n",
      "Iteration  22  completed. loss = 2.9911424826609467\n",
      "Iteration  23  completed. loss = 2.9910807296870576\n",
      "Iteration  24  completed. loss = 2.9910160679247926\n",
      "Iteration  25  completed. loss = 2.990947799778342\n",
      "Iteration  26  completed. loss = 2.990875115932161\n",
      "Iteration  27  completed. loss = 2.990797049446507\n",
      "Iteration  28  completed. loss = 2.990712422334907\n",
      "Iteration  29  completed. loss = 2.9906197777392522\n",
      "Iteration  30  completed. loss = 2.9905172896545476\n",
      "Iteration  31  completed. loss = 2.990402639405743\n",
      "Iteration  32  completed. loss = 2.99027284329057\n",
      "Iteration  33  completed. loss = 2.9901240079816827\n",
      "Iteration  34  completed. loss = 2.989950977615805\n",
      "Iteration  35  completed. loss = 2.989746815788191\n",
      "Iteration  36  completed. loss = 2.9895020312589238\n",
      "Iteration  37  completed. loss = 2.9892033979139625\n",
      "Iteration  38  completed. loss = 2.9888321189016107\n",
      "Iteration  39  completed. loss = 2.988360907601762\n",
      "Iteration  40  completed. loss = 2.987749239898298\n",
      "Iteration  41  completed. loss = 2.9869354520629408\n",
      "Iteration  42  completed. loss = 2.985823291237087\n",
      "Iteration  43  completed. loss = 2.984258572293988\n",
      "Iteration  44  completed. loss = 2.9819881497627208\n",
      "Iteration  45  completed. loss = 2.9785880188931477\n",
      "Iteration  46  completed. loss = 2.9733420157816046\n",
      "Iteration  47  completed. loss = 2.9650615712461645\n",
      "Iteration  48  completed. loss = 2.9519171815716128\n",
      "Iteration  49  completed. loss = 2.9316427781085204\n",
      "Iteration  50  completed. loss = 2.9029719086873484\n",
      "Iteration  51  completed. loss = 2.86852646941697\n",
      "Iteration  52  completed. loss = 2.835520155562372\n",
      "Iteration  53  completed. loss = 2.8105097050529473\n",
      "Iteration  54  completed. loss = 2.7947727633421056\n",
      "Iteration  55  completed. loss = 2.786016932838537\n",
      "Iteration  56  completed. loss = 2.7815129760630923\n",
      "Iteration  57  completed. loss = 2.7793128964362133\n",
      "Iteration  58  completed. loss = 2.778274629885414\n",
      "Iteration  59  completed. loss = 2.77779571732071\n",
      "Iteration  60  completed. loss = 2.7775781199171052\n",
      "Iteration  61  completed. loss = 2.7774802356068875\n",
      "Iteration  62  completed. loss = 2.7774364960146785\n",
      "Iteration  63  completed. loss = 2.7774170388138515\n",
      "Iteration  64  completed. loss = 2.77740840997463\n",
      "Iteration  65  completed. loss = 2.777404591341953\n",
      "Iteration  66  completed. loss = 2.7774029038945245\n",
      "Iteration  67  completed. loss = 2.7774021589668325\n",
      "Iteration  68  completed. loss = 2.7774018303466943\n",
      "Iteration  69  completed. loss = 2.7774016854481656\n",
      "Iteration  70  completed. loss = 2.7774016215793926\n",
      "Iteration  71  completed. loss = 2.7774015934336216\n",
      "Iteration  72  completed. loss = 2.777401581032269\n",
      "Iteration  73  completed. loss = 2.7774015755686823\n",
      "Iteration  74  completed. loss = 2.777401573161802\n",
      "Iteration  75  completed. loss = 2.7774015721015495\n",
      "Iteration  76  completed. loss = 2.7774015716345146\n",
      "Iteration  77  completed. loss = 2.7774015714287934\n",
      "Iteration  78  completed. loss = 2.777401571338178\n",
      "Iteration  79  completed. loss = 2.7774015712982645\n",
      "Iteration  80  completed. loss = 2.777401571280684\n",
      "Iteration  81  completed. loss = 2.7774015712729403\n",
      "Iteration  82  completed. loss = 2.7774015712695292\n",
      "Iteration  83  completed. loss = 2.777401571268027\n",
      "Iteration  84  completed. loss = 2.777401571267365\n",
      "Iteration  85  completed. loss = 2.777401571267074\n",
      "Iteration  86  completed. loss = 2.7774015712669455\n",
      "Iteration  87  completed. loss = 2.777401571266889\n",
      "Iteration  88  completed. loss = 2.777401571266864\n",
      "Iteration  89  completed. loss = 2.777401571266853\n",
      "Iteration  90  completed. loss = 2.7774015712668483\n",
      "Iteration  91  completed. loss = 2.777401571266846\n",
      "Iteration  92  completed. loss = 2.777401571266845\n",
      "Iteration  93  completed. loss = 2.7774015712668447\n",
      "Iteration  94  completed. loss = 2.7774015712668443\n",
      "Iteration  95  completed. loss = 2.7774015712668443\n",
      "Iteration  96  completed. loss = 2.7774015712668443\n",
      "Iteration  97  completed. loss = 2.7774015712668443\n",
      "Iteration  98  completed. loss = 2.7774015712668443\n",
      "Iteration  99  completed. loss = 2.7774015712668443\n",
      "Iteration  100  completed. loss = 2.7774015712668443\n"
     ]
    }
   ],
   "source": [
    "# X = np.array([[[1, -1], [0, 0]], [[0, 0], [1, 1]]])\n",
    "X = np.random.randn(3, 3, 3)\n",
    "# print(X.shape)\n",
    "ans = cpDecomposition(X, 2, maxIter=100, initialisation='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = \n",
      " [[ -3.55787548  -3.19350492]\n",
      " [ -1.05576725 -12.75620143]\n",
      " [  1.05679598  -4.15441272]]\n",
      "B = \n",
      " [[ 0.4074453   0.16899306]\n",
      " [ 0.69986873 -0.1788886 ]\n",
      " [ 0.21453157  0.07221839]]\n",
      "C = \n",
      " [[0.27789052 0.66825859]\n",
      " [0.73250864 0.20892182]\n",
      " [0.83599293 0.07792074]]\n"
     ]
    }
   ],
   "source": [
    "print(\"A = \\n\",ans[0]) \n",
    "print(\"B = \\n\",ans[1])\n",
    "print(\"C = \\n\",ans[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 ('tensors')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b615aee7d680ee864276cc642b4a52cbc17a83f3f1e261f4dcf0c87cb12998c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
